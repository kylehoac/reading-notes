# Reading Notes 17 Course 401

__*Web Scraping*__

Web Scraping is a technique to automatically access and extract large amounts of information from a website.

Notes about web scraping:

1. Read through the websites terms and conditions to understand how to legally use the data.
2. Make sure you dont download data at too rapid a rate because it will cause the website to break

__*Inspecting the website*__

- The first thing you need to do is figure out where to locate the links to the files that you want to download
- You will search through the websites code and find the html tags that correlate to what you're looking to find

__*Python code*__

Import the following

```
import requests
import urllib.request
import time
from bs4 import BeautifulSoup
```

Then set url to the website and use requests library

```
url = 'http://web.mta.info/developers/turnstile.html'
response = requests.get(url)
```

- If the access was successful you should get a 200 error code

Next, parse the html with BeautifulSoup

```
soup = BeautifulSoup(response.text, “html.parser”)
```

Then use the ".findAll" method to locate html tags

```
soup.findAll('a')
```

Extract specific link you want

```
one_a_tag = soup.findAll(‘a’)[38]
link = one_a_tag[‘href’]
```

Finally use urllib.request library to download the file path

```
download_url = 'http://web.mta.info/developers/'+ link
urllib.request.urlretrieve(download_url,'./'+link[link.find('/turnstile_')+1:])
```

- use time.sleep(1) in order to set an interval so that you arent downloading files too rapidly

__*Different ways to scrape the web*__

Human copy and paste - Simplest form of web scraping. Manual

Text pattern Matching - Based on the UNIX grep command or regular expression matching

HTTP programming - Posting HTTP requests to a remote web server using socket programming

HTML Parsing - Extracts, and translates content that is encoded into pages by use of a common script or template

DOM Parsing - Using browser controls to parse web parse web pages into a DOM tree

Verticle Aggregation - Creating and using bots for specific verticles

Semantic Annotation Recognizing - Using metadata, semantic markups, and annotations to locate specific data snippets

Computer Vision web-page analysis - Using machine learning to identify information from web pages by interpreting pages the way a human might

__*Steps to take to ensure you don't get blocked*__

1. Respect the Robots.txt file
2. Make the crawler slower so that you don't harm the server
3. Don't follow the same crawling pattern
4. Make requests using proxies, and rotate them
5. Rotate user agents and HTTP request headers between requests
6. Use a headless browser
7. Beware of honeypot traps
8. Check if the website is changing layouts
9. Avoid scraping behind a login
10. Use captcha solving services

[<== Back to Main Page](README.md)
